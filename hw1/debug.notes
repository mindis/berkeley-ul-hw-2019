Note to change factorised / full in your model change get_pixelcnn_mask() default kwarg in pixelCNN.py

PixelCNN
    Factorised
        works on debug and few (single)
        Main 4k iter, loss bit jumpy, try lower LR? samples getting there but poor
    Full
        works on debug, bit dubious but seems to learn something on few (single)
        Main w/ 4,000 iterations -

PixelCNN-MADE
    Factorised
        works on debug and few (single)
        main 4k: loss improves but doesn't get past 0.33, samples poor
    Full
        main 4k: loss improves but doesn't get past 0.33, samples poor but do seem to be getting there

PixelCNN-DS
on main, 10e-4, lowest loss: 0.134 in 2,500 iterations, samples good quality
why is this lower than they report?
    x DONE try batching that shuffles then goes through all examples
    performs better on new batching, more monotonic decrease of loss, samples are good
10e-3 on theirs
    seems noisy and jumpy, samples quite poor

Swapping in masking functions
    Their masking function in my model (pixelcnn) gets to 0.133 loss, but samples are poor
        -> perhaps its my mask AND model? my sampling?
    My masking in their model, seems to improve then at 500 iter at 0.33 loss it blows up to 1+ loss for the rest
    samples are v poor
        -> my masking is wrong?
    Their mask and samples in my model: loss to 0.18, samples are better than with my sampling, but still not as
    good as their full setup - has the texture but not the shape
        -> look at sampling, but have also changed softmax now

13/01
Have changed my architecture to match the paper and DS code (removed one of the 1x1 output conv layers)
Have read through the code and mostly seems similar to mine, have made note below of a few things to check:
    DONE compare the masking
TFP and NP sampling is empirically very similar (see PixelCNN_DS.compare_sampling)

14/01
Masks mine vs DS has different ordering of channels in and out to corresponding masks.
Mine is R, G, B, R, G, B ..., DS's is R, R, R.., G, G, G.., B, B, B..
This could be the issue, perhaps when reshaped it then doesn't match the channels ordered
as it's done it masking function.
This seems to be the issue, see pixelCNN_DS.tf_reshape_masks, here the plots show for one
input channel, the output channel in C x N grid. These should repeat as it's R in to the R, G, B
channels and each of N values they can take. Mine don't repeat so think this is an issue
My masking makes sense when rehshaping to (bs, K, K, N, C) but DS makes sense in the actual
usage to (bs, K, K, C, N).


sort factorised=True masking
    should have been same with my bug masking and potential bug sampling
    Could also be something in model - LR?
    Loss does seem to get there - 0.18 at 1900, it's just jumpy
    with lower lR (5e-4) - still jumpy loss decrease - why?
    try look at model for differences

running new masking on full
    loss gets down to 0.15 in 2000, samples not great but are getting there
    final loss in 4k, 0.154, samples seem to have some quality and semblance but not quite there yet -> debug

Note the best loss I've had is 0.13 but DS's gets to 0.127 (and less than 0.05 with MADE)

next:
    * masking with ints vs floats (factorised) to compare if same 
    * compare convs
    * figure out why factorised model not good as it can be (jumpy loss)
    * figure out why full model samples not great

06/02/2020

Checked pixelCNN_DS.py for seed / repeatability, updated to make so
Checked pixelCNN.py for seed / repeatability, PASSED
Also checked TF-probability and Numpy
PixelCNN_DS - reapeatable training loss PASSED
PixelCNN - reapeatable training loss PASSED

08/02/2020

Note here used both mine and DS with factorised=True

Updated training logger and plotting
Checked DS code with github and it's as is but adapted for TF versions

Initially, my model slower - DS does 4k iter in 1h on PC
Have optimised performance now and it's quicker than DS

Benchmarked my model and DS model as they are now
My model better with lower LR, but overall the losses seem similar on mine and DS, samples seem worse in mine

Ran mine with numpy sampling and the sample quality no better.
But only the sampling was changed and yet the losses are different. This could be
from GPU or is change needed for repeatability?

Trying mine and DS as are to benchmark w factorised=False

Also ran both factorised = False
Both achieve lower loss that with factorised = True
Again, v similar loss values but samples better in DS code

Adding my code to DS:
1st - masking (factorised = True)
Loss jumps at end and samples get v poor, but most samples during training are similar to DS

Looking at it with more fresh eyes, I do think the sample quality is actually quite similar
between mine and DS.
Will do some more sanity checks on masking to be sure.

Did with lower LR (2.5e-4) and no better loss or samples (on mine factorised=True)

Matched 1000 masks with random parameters between mine and DS masking functions.

DS code as is (with their training loop) runs better. Similar loss but better samples.
So going to look at adapting that into mine.

Improved the training loop.
Doesn't initially appear to improve mine. (factorised=False)
Jumpy loss, and still not quite there on sample quality
Could be LR? I think still a bug

Worked DS code into the new training loop.
Had to adapt it to work with TF2 eg. functions not sessions, so may have introduced some bugs.
Seems to be working OK now.
Report of DS on new training loop (factorised=False):
Not great, loss initially get better but then gets worse again and samples v poor
Noticed that original DS training loop uses 10e-4 I was using 10e-3 so try again with that LR

15th Feb 2020

Trying DS training loop with adapted TF 2 DS code
Loss got v low 0.075
Sample quality really good.
This is the same code as DS before just different training loop -> I think your issue could be
optimisation not a bug!?

Got DS code working with my training loop
"Working" ie. same loss and sample quality as with DS full code from git

Ran mine now with 10e-4 and grad clipping
Got loss to 0.055 so similar to DS code
Samples I'd say they're worse still, but comparable quality

27th, 28th Feb

Ran mine with 10e-4 LR and factorised=True, loss to 0.12, not as low as factorised=False but samples v good
I am happy to move onto PixelCNN-MADE and then onwards :)!

PixelCNN-MADE
Adapting PixelCNN-MADE since updated code and PixelCNN
Made sure MADE still works!

PixelCNN-MADE factorised=True loss only to 0.31 and samples poor
PixelCNN-MADE factorised=False loss only to 0.33 and samples poor

PixelCNN: Note from HW that should be AR over space but factorised over channels
This doesn't affect loss since the masking enforces the conditioning there.
In sampling factorised we need only 1 forward pass we then sample all 3 channels as they are independent
I have implemented this and tested it works
PixelCNN-MADE should only use unfactorised since we want MADE to capture dependencies between channels

- Unit ordering
Inputs and outputs need to be ordered such that it matches with the PixelCNN conditioning. O/w the order
of the variables conflicts for sampling
For D variables (H x W x C) taking N values
For inputs and outputs:
We need the unit numbers to be 1 for the first N then 2 for the next N ... until D for the last N
np.repeat(np.arange(H*W*C), N).reshape([H,W,C,N]) seems to give right ordering

Shouldn't need to one-hot because the loss in MADE uses sparse Xent.

Ordered MADE works on q2, the final distribution is empirically good
But the loss doesn't get as low as before
Think this is OK since the other one train loss was much lower than validation loss so was overfitting I think

Removed DS code!

PixelCNN-MADE loss quite high (0.7) and samples v poor

Running with np.tile instead of np.repeat but already reverted code since don't think it's this
NO

DS code uses factorised for PixelCNN-MADE

Using
    rep = int(np.ceil(nrof_units / ((nrof_dims - 1))))
    layer_units = np.repeat(np.arange(1, nrof_dims), rep)[:nrof_units]
    order_units = ordered_unit_number(nrof_dims, nrof_bins)
Then
    mask = get_mask_made(order_units, layer_units)
Gives same input - 1st layer masks as DS
    mask = get_mask_made(layer_units, layer_units)
(w/ above order_layer_units) gives same hidden to hidden masks as DS
    mask = get_mask_made(layer_units, order_units-1)
Gives same output layer masks as DS

Had error in sampled MADE unit numbers going to D+1 when should go to D (exclusively, so max is D-1)
Meant some output layers were hidden to outs had whole rows as 0s in masks
Might be improved now resolved that bug?

It's ok with D+1 in ordered unit numbers for input and output layers since these are 1 to D (so D+1 exclusive)
Though the output layer we then -1 to make the inequality strict


Tried factorised
NO, but did work better than not factorised

DS seems to be a MADE over channels ie. D = C, N = 4 not over pixels like yours with D=H*W*C, N = 4
But how does MADE part know which pixel we are on? Seems to take each pixel as a C x N ie. 3 x 4 vector of
one hot discrete values for each channel.

03rd March 2020
BIG CHANGE to pixelCNN-MADE
Realised I'm going about this completely wrong!
Should be a MADE over channels for each pixel (each pixel like a data point), then use the pixelCNN as auxiliary
input

Mask on aux for ith pixel should be 1s up to (not incld) i then 0s
I suppose we concat this to the C vector input for the pixel's channel values which we MADE over?
UPDATE:
I'm now thinking it's that we have inputs D * N (where D = C here for a value for each channel) from the image data (1-hot)
then D * N values for that pixel from pixelCNN outputs. This seems to match DS code and the idea they give in the
assignment, but is this the right aux vector input from pixelCNN, it says for x1 to i-1 which makes me think
a D*N vector each time is wrong and we need a H*W*C vector then masked for up to the current pixel?

Aux vars
handle aux vars.
Could be in MADE.forward_logits() since this is called by all forward passes
    Here would be separate input with potential None value if no aux input. This makes it explicit.
Could be in model but this seems unnecessary since if do upstream then  Model then acts as though it's a larger input.
Could be in PixelCNNMADEModel, this works well as minimal changes to MADE and since the aux input comes from
PixelCNN can all be handled in this class.
Doesn't make it as explicit for other uses that might want aux vars MADE but works well and cleanly for this use.
Add to pixelCNNMadeModel
Concat aux with x then input to model.

Handle aux vars (DONE)
One hot inputs to MADE (not the pixelCNN) (DONE)

Ran on main, loss only to 0.5 and
outputs are v uniform, all pixels are two or few colours, seems to be outputing average for most pixels
Perhaps I got the setup wrong here with the aux input or the input to MADE?

DS has diff masking,
also adds aux input to each layer

tried with concat other way
Not solved it

Tried diff masking in my code that matches DS (see above for code)
Doesnt look to be it

Tried aux input to each layer
Not solved it

Have got my pixelcnn then DS' MADE working on debug
Tried on main data
Good loss 0.11, but v poor samples, all green
Think this maybe to do with masking? As it can get low loss by seeing other pixels but when
sampling these are zeros so poor samples
Or maybe bc of the order of the concat seems wrong in DS code?

Tried with factorised=True instead
Loss decreases to 0.2 but then jumps back to 0.8 and stays there, and samples poor
So tried with lower LR, factorised=True
Loss gets to 0.2 then a bit jumpy and doesnt seem to get below that, samples better than with factorised=False
bc they're not all green but not resembling the right digit shapes yet

Tried same as experiment that worked with my PixelCNN and DS MADE (factroised, lower LR) with my MADE
No, loss to 0.6 after 2k

Tried other way of concat aux and x when pass to MADE with DS MADE and my pixelCNN
With factorised = True
    Loss gets really low, 0.04 in 1k but samples v poor
With fac=False
    Same problem
So I guess this is wrong masking due to the low loss but poor samples? It cheats and looks at
other pixel values but then when we sample the other pixel values are random / zero

Comparing DS MADE masking to mine using ordered version of sampling unit numbers all same WITHOUT AUX
Comparing my masking to DS with AUX again seems same except DS uses 1's on aux ie. all unmasked and inputs the aux
every layer
Looked again at MADE paper and saw some bugs, changed code, worth running MADE and trying pixelCNN MADE again

With DS masking in my MADE and PixelCNN still gets stuck at 0.5 loss and samples poor
-> So could be some other supporting code, look for differences
With my masking (random) and my MADE and pixelCNN same result, 0.5 loss

Tried DS MADE code with my pixelCNN layers and masks
Loss stuck around 0.35-0.4 and all green samples
So it is my MADE layers that's the issue and/or supporting code
One other diff here is that changed to not aux in each layer only concat to inputs to MADE

Only difference with my masking seems to be that the aux input on mine has unit numbers ie some are masked whereas
with DS code they all unmasked

DS support code with my MADE layers and pixelcNN and to not have aux input each layer
Seems to have same issue, 0.35 loss
.
But try with aux and x concat other way bc this is important with masking
Better than above, loss to 0.15 and decent samples
-> Look into this on my pixel CNN MADE and why?

My PixelCNN + MADE with aux and x concat other way
Ran this twice and had 0.5 loss and bad samples still - find the bug!
Given above experiment the only thing left is the MADE suppporting code since the MADE layer and my
pixelCNN are used in DS code now

Tried DS code with my MADEModel and PixelCNN Model in DSPixelCNNMADEModel
if this doesn't work then it's something in MADE Model
if it does then it points to the issue in supporting code
Ok, so it didnt work therefore I think best place to look is in MADEModel
.
Ran my code with DS_MADEModel just to confirm as this uses MADELayer but not MADEModel
This didnt work so I think there's multiple issues
.
Ran DS code with my full PixelCNNMADEModel, so it's just DS supporting code
0.7 loss
->
So it's something in my PixelCNNMADEModel or MADEModel
It's NOT supporting code or MADELayer

Trying MADEModel without passing output unit numbers as seems to be one difference in MADEModel and DS
No
Trying with flat=False in pixelCNNModel within PixelCNNMADEModel
No

Debugging from DS code then integrating mine
Seem to have DS code working

Tried DS with aux only in input layer and didn't really get there
.
Tried again with more units on MADE layers (132)
No, but maybe not a big enough change for noticeable difference

Running my code as is, if it doesnt work try changing some hparams
LR, n units etc.

Looking at this other guy's code too https://github.com/ikrets/CS294-158-homeworks/blob/master/1/HW1_3_25epochs.ipynb
Adapted his to keras model to fit my training code
Ran with my pixelCNN and ikrets' MADE on top, works fine so definitely something in the MADE part

With my adapted ikrets MADE layers in his training code w my pixelCNN gets stuck at 0.3
Think there's a bug in the adapted layers?
Also seems to imply there's a bug in the training code or model code for my made_ds2

Changed my adapted ikrets code, wasnt sending aux to the hidden MADE layer
Now it gets to 0.1 loss

So difference to adapted layers in my training loop is something in the model or training code
Ikrets code works with my pixelCNN and my training loop, so the bug is in the MADE!

Ikrets seems to be MADE output the whole image not just the current pixel

* Try decipher what ikrets einsum is doing!
* first try get basic working even if bad, then try improve with tricks
* print intermediary values in yours, adapted and git version of ikrets
* relu after pixelCNN
* Look at embeddings from pixelCNN that are input as aux
* Some res connections in MADE eg. Aux input to the unconnected unit in final layer MADE
* aux input each layer

* Debug new pixelcnn-made
    * Look my MADE masking cf DS MADE masking for pixelCNN
* remove ds code and remove ordered=True from MADE
- Factorised=True seems important, not sure why?
* Clean up Ds and Ns etc
* Run gradient thing part 3
* Sort README
    About factorised flag on pixelCNN
