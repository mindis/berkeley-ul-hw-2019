Note to change factorised / full in your model change get_pixelcnn_mask() default kwarg in pixelCNN.py

PixelCNN
    Factorised
        works on debug and few (single)
        Main 4k iter, loss bit jumpy, try lower LR? samples getting there but poor
    Full
        works on debug, bit dubious but seems to learn something on few (single)
        Main w/ 4,000 iterations -

PixelCNN-MADE
    Factorised
        works on debug and few (single)
        main 4k: loss improves but doesn't get past 0.33, samples poor
    Full
        main 4k: loss improves but doesn't get past 0.33, samples poor but do seem to be getting there

PixelCNN-DS
on main, 10e-4, lowest loss: 0.134 in 2,500 iterations, samples good quality
why is this lower than they report?
    x DONE try batching that shuffles then goes through all examples
    performs better on new batching, more monotonic decrease of loss, samples are good
10e-3 on theirs
    seems noisy and jumpy, samples quite poor

Swapping in masking functions
    Their masking function in my model (pixelcnn) gets to 0.133 loss, but samples are poor
        -> perhaps its my mask AND model? my sampling?
    My masking in their model, seems to improve then at 500 iter at 0.33 loss it blows up to 1+ loss for the rest
    samples are v poor
        -> my masking is wrong?
    Their mask and samples in my model: loss to 0.18, samples are better than with my sampling, but still not as
    good as their full setup - has the texture but not the shape
        -> look at sampling, but have also changed softmax now

13/01
Have changed my architecture to match the paper and DS code (removed one of the 1x1 output conv layers)
Have read through the code and mostly seems similar to mine, have made note below of a few things to check:
    DONE compare the masking
TFP and NP sampling is empirically very similar (see PixelCNN_DS.compare_sampling)

14/01
Masks mine vs DS has different ordering of channels in and out to corresponding masks.
Mine is R, G, B, R, G, B ..., DS's is R, R, R.., G, G, G.., B, B, B..
This could be the issue, perhaps when reshaped it then doesn't match the channels ordered
as it's done it masking function.
This seems to be the issue, see pixelCNN_DS.tf_reshape_masks, here the plots show for one
input channel, the output channel in C x N grid. These should repeat as it's R in to the R, G, B
channels and each of N values they can take. Mine don't repeat so think this is an issue
My masking makes sense when rehshaping to (bs, K, K, N, C) but DS makes sense in the actual
usage to (bs, K, K, C, N).


sort factorised=True masking
    should have been same with my bug masking and potential bug sampling
    Could also be something in model - LR?
    Loss does seem to get there - 0.18 at 1900, it's just jumpy
    with lower lR (5e-4) - still jumpy loss decrease - why?
    try look at model for differences

running new masking on full
    loss gets down to 0.15 in 2000, samples not great but are getting there
    final loss in 4k, 0.154, samples seem to have some quality and semblance but not quite there yet -> debug

Note the best loss I've had is 0.13 but DS's gets to 0.127 (and less than 0.05 with MADE)

next:
    * masking with ints vs floats (factorised) to compare if same 
    * compare convs
    * figure out why factorised model not good as it can be (jumpy loss)
    * figure out why full model samples not great

06/02/2020

Checked pixelCNN_DS.py for seed / repeatability, updated to make so
Checked pixelCNN.py for seed / repeatability, PASSED
Also checked TF-probability and Numpy
PixelCNN_DS - reapeatable training loss PASSED
PixelCNN - reapeatable training loss PASSED

08/02/2020

Note here used both mine and DS with factorised=True

Updated training logger and plotting
Checked DS code with github and it's as is but adapted for TF versions

Initially, my model slower - DS does 4k iter in 1h on PC
Have optimised performance now and it's quicker than DS

Benchmarked my model and DS model as they are now
My model better with lower LR, but overall the losses seem similar on mine and DS, samples seem worse in mine

Ran mine with numpy sampling and the sample quality no better.
But only the sampling was changed and yet the losses are different. This could be
from GPU or is change needed for repeatability?

Trying mine and DS as are to benchmark w factorised=False

Also ran both factorised = False
Both achieve lower loss that with factorised = True
Again, v similar loss values but samples better in DS code

Adding my code to DS:
1st - masking (factorised = True)
Loss jumps at end and samples get v poor, but most samples during training are similar to DS

Looking at it with more fresh eyes, I do think the sample quality is actually quite similar
between mine and DS.
Will do some more sanity checks on masking to be sure.

Did with lower LR (2.5e-4) and no better loss or samples (on mine factorised=True)

Matched 1000 masks with random parameters between mine and DS masking functions.

DS code as is (with their training loop) runs better. Similar loss but better samples.
So going to look at adapting that into mine.

Improved the training loop.
Doesn't initially appear to improve mine. (factorised=False)
Jumpy loss, and still not quite there on sample quality
Could be LR? I think still a bug

Worked DS code into the new training loop.
Had to adapt it to work with TF2 eg. functions not sessions, so may have introduced some bugs.
Seems to be working OK now.
Report of DS on new training loop (factorised=False):
Not great, loss initially get better but then gets worse again and samples v poor
Noticed that original DS training loop uses 10e-4 I was using 10e-3 so try again with that LR

15th Feb 2020

Trying DS training loop with adapted TF 2 DS code
Loss got v low 0.075
Sample quality really good.
This is the same code as DS before just different training loop -> I think your issue could be
optimisation not a bug!?

Got DS code working with my training loop
"Working" ie. same loss and sample quality as with DS full code from git

Ran mine now with 10e-4 and grad clipping
Got loss to 0.055 so similar to DS code
Samples I'd say they're worse still, but comparable quality

27th, 28th Feb

Ran mine with 10e-4 LR and factorised=True, loss to 0.12, not as low as factorised=False but samples v good
I am happy to move onto PixelCNN-MADE and then onwards :)!

PixelCNN-MADE
Adapting PixelCNN-MADE since updated code and PixelCNN
Made sure MADE still works!

PixelCNN-MADE factorised=True loss only to 0.31 and samples poor
PixelCNN-MADE factorised=False loss only to 0.33 and samples poor

PixelCNN: Note from HW that should be AR over space but factorised over channels
This doesn't affect loss since the masking enforces the conditioning there.
In sampling factorised we need only 1 forward pass we then sample all 3 channels as they are independent
I have implemented this and tested it works
PixelCNN-MADE should only use unfactorised since we want MADE to capture dependencies between channels

- Unit ordering
Inputs and outputs need to be ordered such that it matches with the PixelCNN conditioning. O/w the order
of the variables conflicts for sampling
For D variables (H x W x C) taking N values
For inputs and outputs:
We need the unit numbers to be 1 for the first N then 2 for the next N ... until D for the last N
np.repeat(np.arange(H*W*C), N).reshape([H,W,C,N]) seems to give right ordering

Shouldn't need to one-hot because the loss in MADE uses sparse Xent.

Ordered MADE works on q2, the final distribution is empirically good
But the loss doesn't get as low as before
Think this is OK since the other one train loss was much lower than validation loss so was overfitting I think

Removed DS code!

PixelCNN-MADE loss quite high (0.7) and samples v poor

Running with np.tile instead of np.repeat but already reverted code since don't think it's this
NO

DS code uses factorised for PixelCNN-MADE

Using
    rep = int(np.ceil(nrof_units / ((nrof_dims - 1))))
    layer_units = np.repeat(np.arange(1, nrof_dims), rep)[:nrof_units]
    order_units = ordered_unit_number(nrof_dims, nrof_bins)
Then
    mask = get_mask_made(order_units, layer_units)
Gives same input - 1st layer masks as DS
    mask = get_mask_made(layer_units, layer_units)
(w/ above order_layer_units) gives same hidden to hidden masks as DS
    mask = get_mask_made(layer_units, order_units-1)
Gives same output layer masks as DS

Had error in sampled MADE unit numbers going to D+1 when should go to D (exclusively, so max is D-1)
Meant some output layers were hidden to outs had whole rows as 0s in masks
Might be improved now resolved that bug?

It's ok with D+1 in ordered unit numbers for input and output layers since these are 1 to D (so D+1 exclusive)
Though the output layer we then -1 to make the inequality strict


Tried factorised
NO, but did work better than not factorised

DS seems to be a MADE over channels ie. D = C, N = 4 not over pixels like yours with D=H*W*C, N = 4
But how does MADE part know which pixel we are on? Seems to take each pixel as a C x N ie. 3 x 4 vector of
one hot discrete values for each channel.

03rd March 2020
BIG CHANGE to pixelCNN-MADE
Realised I'm going about this completely wrong!
Should be a MADE over channels for each pixel (each pixel like a data point), then use the pixelCNN as auxiliary
input

Mask on aux for ith pixel should be 1s up to (not incld) i then 0s
I suppose we concat this to the C vector input for the pixel's channel values which we MADE over?
UPDATE:
I'm now thinking it's that we have inputs D * N (where D = C here for a value for each channel) from the image data (1-hot)
then D * N values for that pixel from pixelCNN outputs. This seems to match DS code and the idea they give in the
assignment, but is this the right aux vector input from pixelCNN, it says for x1 to i-1 which makes me think
a D*N vector each time is wrong and we need a H*W*C vector then masked for up to the current pixel?

Aux vars
handle aux vars.
Could be in MADE.forward_logits() since this is called by all forward passes
    Here would be separate input with potential None value if no aux input. This makes it explicit.
Could be in model but this seems unnecessary since if do upstream then  Model then acts as though it's a larger input.
Could be in PixelCNNMADEModel, this works well as minimal changes to MADE and since the aux input comes from
PixelCNN can all be handled in this class.
Doesn't make it as explicit for other uses that might want aux vars MADE but works well and cleanly for this use.
Add to pixelCNNMadeModel
Concat aux with x then input to model.

Handle aux vars (DONE)
One hot inputs to MADE (not the pixelCNN) (DONE)

Ran on main, loss only to 0.5 and
outputs are v uniform, all pixels are two or few colours, seems to be outputing average for most pixels
Perhaps I got the setup wrong here with the aux input or the input to MADE?

DS has diff masking,
also adds aux input to each layer

tried with concat other way
Not solved it

Try diff masking that matches DS (see above for code)
Doesnt look to be it

* Debug new pixelcnn-made
    * try aux input each layer
    * try some of DS code
* Clean up Ds and Ns etc
* Run gradient thing part 3
* Sort README
    About factorised flag on pixelCNN