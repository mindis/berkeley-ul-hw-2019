Note to change factorised / full in your model change get_pixelcnn_mask() default kwarg in pixelCNN.py

PixelCNN
    Factorised
        works on debug and few (single)
        Main 4k iter, loss bit jumpy, try lower LR? samples getting there but poor
    Full
        works on debug, bit dubious but seems to learn something on few (single)
        Main w/ 4,000 iterations -

PixelCNN-MADE
    Factorised
        works on debug and few (single)
        main 4k: loss improves but doesn't get past 0.33, samples poor
    Full
        main 4k: loss improves but doesn't get past 0.33, samples poor but do seem to be getting there

PixelCNN-DS
on main, 10e-4, lowest loss: 0.134 in 2,500 iterations, samples good quality
why is this lower than they report?
    x DONE try batching that shuffles then goes through all examples
    performs better on new batching, more monotonic decrease of loss, samples are good
10e-3 on theirs
    seems noisy and jumpy, samples quite poor

Swapping in masking functions
    Their masking function in my model (pixelcnn) gets to 0.133 loss, but samples are poor
        -> perhaps its my mask AND model? my sampling?
    My masking in their model, seems to improve then at 500 iter at 0.33 loss it blows up to 1+ loss for the rest
    samples are v poor
        -> my masking is wrong?
    Their mask and samples in my model: loss to 0.18, samples are better than with my sampling, but still not as
    good as their full setup - has the texture but not the shape
        -> look at sampling, but have also changed softmax now

13/01
Have changed my architecture to match the paper and DS code (removed one of the 1x1 output conv layers)
Have read through the code and mostly seems similar to mine, have made note below of a few things to check:
    DONE compare the masking
TFP and NP sampling is empirically very similar (see PixelCNN_DS.compare_sampling)

14/01
Masks mine vs DS has different ordering of channels in and out to corresponding masks.
Mine is R, G, B, R, G, B ..., DS's is R, R, R.., G, G, G.., B, B, B..
This could be the issue, perhaps when reshaped it then doesn't match the channels ordered
as it's done it masking function.
This seems to be the issue, see pixelCNN_DS.tf_reshape_masks, here the plots show for one
input channel, the output channel in C x N grid. These should repeat as it's R in to the R, G, B
channels and each of N values they can take. Mine don't repeat so think this is an issue
My masking makes sense when rehshaping to (bs, K, K, N, C) but DS makes sense in the actual
usage to (bs, K, K, C, N).


sort factorised=True masking
    should have been same with my bug masking and potential bug sampling
    Could also be something in model - LR?
    Loss does seem to get there - 0.18 at 1900, it's just jumpy
    with lower lR (5e-4) - still jumpy loss decrease - why?
    try look at model for differences

running new masking on full
    loss gets down to 0.15 in 2000, samples not great but are getting there
    final loss in 4k, 0.154, samples seem to have some quality and semblance but not quite there yet -> debug

Note the best loss I've had is 0.13 but DS's gets to 0.127 (and less than 0.05 with MADE)

next:
    * masking with ints vs floats (factorised) to compare if same 
    * compare convs
    * figure out why factorised model not good as it can be (jumpy loss)
    * figure out why full model samples not great

06/02/2020

Checked pixelCNN_DS.py for seed / repeatability, updated to make so
Checked pixelCNN.py for seed / repeatability, PASSED
Also checked TF-probability and Numpy
PixelCNN_DS - reapeatable training loss PASSED
PixelCNN - reapeatable training loss PASSED

08/02/2020

Note here used both mine and DS with factorised=True

Updated training logger and plotting
Checked DS code with github and it's as is but adapted for TF versions

Initially, my model slower - DS does 4k iter in 1h on PC
Have optimised performance now and it's quicker than DS

Benchmarked my model and DS model as they are now
My model better with lower LR, but overall the losses seem similar on mine and DS, samples seem worse in mine

Ran mine with numpy sampling and the sample quality no better.
But only the sampling was changed and yet the losses are different. This could be
from GPU or is change needed for repeatability?

Trying mine and DS as are to benchmark w factorised=False

Also ran both factorised = False
Both achieve lower loss that with factorised = True
Again, v similar loss values but samples better in DS code

Adding my code to DS:
1st - masking (factorised = True)
Loss jumps at end and samples get v poor, but most samples during training are similar to DS

Looking at it with more fresh eyes, I do think the sample quality is actually quite similar
between mine and DS.
Will do some more sanity checks on masking to be sure.

Did with lower LR (2.5e-4) and no better loss or samples (on mine factorised=True)

Matched 1000 masks with random parameters between mine and DS masking functions.

DS code as is (with their training loop) runs better. Similar loss but better samples.
So going to look at adapting that into mine.

Improved the training loop.
Doesn't initially appear to improve mine. (factorised=False)
Jumpy loss, and still not quite there on sample quality
Could be LR? I think still a bug

Worked DS code into the new training loop.
Had to adapt it to work with TF2 eg. functions not sessions, so may have introduced some bugs.
Seems to be working OK now.
Report of DS on new training loop (factorised=False):
Not great, loss initially get better but then gets worse again and samples v poor
Noticed that original DS training loop uses 10e-4 I was using 10e-3 so try again with that LR

Trying DS training loop with adapted TF 2 DS code
Loss got v low 0.065
Sampling: ... TODO ...

- is there something else that needs seeding / repeatability
- my model prefers lower LR - why??
- My samples seem worse though loss similar - why?

* Try adapted model back with original training loop
* Try full run on 5e-4 LR mine? Try on DS on 10e-4 my train loop (bc this is what DS code used!)?
* Further compare and debug: add some more of my code to DS and run again
* see other TODOs above and comments

