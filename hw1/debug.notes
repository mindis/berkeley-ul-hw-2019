Note to change factorised / full in your model change get_pixelcnn_mask() default kwarg in pixelCNN.py

PixelCNN
    Factorised
        works on debug and few (single)
        Main 4k iter, loss bit jumpy, try lower LR? samples getting there but poor
    Full
        works on debug, bit dubious but seems to learn something on few (single)
        Main w/ 4,000 iterations -
2* try 4k iter on full

PixelCNN-MADE
    Factorised
        works on debug and few (single)
        main 4k: loss improves but doesn't get past 0.33, samples poor
    Full
        main 4k: loss improves but doesn't get past 0.33, samples poor but do seem to be getting there

PixelCNN-DS
on main, 10e-4, lowest loss: 0.134 in 2,500 iterations, samples good quality
why is this lower than they report?
    x DONE try batching that shuffles then goes through all examples
    performs better on new batching, more monotonic decrease of loss, samples are good
10e-3 on theirs
    seems noisy and jumpy, samples quite poor
* compare and debug to yours
3* swap in parts eg. masking function

Swapping in masking functions
    Their masking function in my model (pixelcnn) gets to 0.133 loss, but samples are poor
        -> perhaps its my mask AND model? my sampling?
    My masking in their model, seems to improve then at 500 iter at 0.33 loss it blows up to 1+ loss for the rest
    samples are v poor
        -> my masking is wrong?
    Their mask and samples in my model: loss to 0.18, samples are better than with my sampling, but still not as
    good as their full setup - has the texture but not the shape
        -> look at sampling, but have also changed softmax now

13/01
Have changed my architecture to match the paper and DS code (removed one of the 1x1 output conv layers)
Have read through the code and mostly seems similar to mine, have made note below of a few things to check:
    DONE compare the masking
    * any difference in the conv I use vs DS?
TFP and NP sampling is empirically very similar (see PixelCNN_DS.compare_sampling)

14/01
Masks mine vs DS has different ordering of channels in and out to corresponding masks.
Mine is R, G, B, R, G, B ..., DS's is R, R, R.., G, G, G.., B, B, B..
This could be the issue, perhaps when reshaped it then doesn't match the channels ordered
as it's done it masking function.
This seems to be the issue, see pixelCNN_DS.tf_reshape_masks, here the plots show for one
input channel, the output channel in C x N grid. These should repeat as it's R in to the R, G, B
channels and each of N values they can take. Mine don't repeat so think this is an issue
My masking makes sense when rehshaping to (bs, K, K, N, C) but DS makes sense in the actual
usage to (bs, K, K, C, N).


sort factorised=True masking
    should have been same with my bug masking and potential bug sampling
    Could also be something in model - LR?
    Loss does seem to get there - 0.18 at 1900, it's just jumpy
    with lower lR (5e-4) - still jumpy loss decrease - why?
    try look at model for differences
        * grad clipping?
        * int vs float in mask a bug? try int and float to compare.

running new masking on full
    loss gets down to 0.15 in 2000, samples not great but are getting there
    final loss in 4k, 0.154, samples seem to have some quality and semblance but not quite there yet -> debug
    * debug

Note the best loss I've had is 0.13 but DS's gets to 0.127 (and less than 0.05 with MADE)

next:
    1* fix the seeds - currently not repeatable
    * masking with ints vs floats (factorised) to compare if same 
    * compare convs (mark as DONE above when done)
    * figure out why factorised model not good as it can be (jumpy loss)
    * figure out why full model samples not great

06/02/2020

Checked pixelCNN_DS.py for seed / repeatability, updated to make so
Checked pixelCNN.py for seed / repeatability, PASSED
Also checked TF-probability and Numpy
* Only checked the weight initialisation, may be other parts needing repeatability, needs further testing
* Then add my code incrementally to DS code and debug

